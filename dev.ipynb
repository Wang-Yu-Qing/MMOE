{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:30:24.845137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, tf.Tensor): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def buildSample(fea, matrital, income):\n",
    "    data = {\n",
    "        'fea': _bytes_feature(tf.io.serialize_tensor(fea)),\n",
    "        'marital': _int64_feature(matrital),\n",
    "        'income': _int64_feature(income)\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "\n",
    "    return example\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.strip().split(\",\")\n",
    "    return int(fields[0]), int(fields[1]), [float(x) for x in fields[2:]]\n",
    "\n",
    "# save as tfrecord to disk, or use `tf.data.Dataset.from_tensor_slices` in memory\n",
    "train_data_path = \"data/census/tfrecords/train.tfrecords\"\n",
    "test_data_path = \"data/census/tfrecords/test.tfrecords\"\n",
    "#with tf.io.TFRecordWriter(train_data_path) as writer:\n",
    "#    with open(\"data/census/train_data.csv\", \"r\") as f:\n",
    "#        for line in f.readlines():\n",
    "#            marital, income, fea = parseLine(line)\n",
    "#            example = buildSample(fea, marital, income)\n",
    "#            writer.write(example.SerializeToString())\n",
    "#\n",
    "#with tf.io.TFRecordWriter(test_data_path) as writer:\n",
    "#    with open(\"data/census/test_data.csv\", \"r\") as f:\n",
    "#        for line in f.readlines():\n",
    "#            marital, income, fea = parseLine(line)\n",
    "#            example = buildSample(fea, marital, income)\n",
    "#            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMOEDense(object):\n",
    "    def __init__(self, nTasks, nExperts, inputDim, expertDim, hiddenDim, lr=0.01):\n",
    "        self.nTasks = nTasks\n",
    "        self.nExperts = nExperts\n",
    "        self.inputDim = inputDim\n",
    "        self.expertDim = expertDim\n",
    "        # experts ops are independent, so don't use loop\n",
    "        # experts (nExperts, inputDim, expertDim)\n",
    "        expertInit = tf.initializers.truncated_normal(mean=0.0, stddev=1.0)\n",
    "        self.experts = tf.Variable(expertInit(shape=(nExperts, inputDim, expertDim), dtype=tf.float32), name=\"experts\")\n",
    "        # gates, (nTasks, inputDim, nExpert)\n",
    "        gateInit = tf.initializers.truncated_normal(mean=0.0, stddev=1.0)\n",
    "        self.gates = tf.Variable(gateInit(shape=(nTasks, inputDim, nExperts), dtype=tf.float32), name=\"gates\")\n",
    "        # towers' mlp for each task, (nTasks, expertDim, hiddenDim)\n",
    "        towersInit = tf.initializers.truncated_normal(mean=0.0, stddev=1.0)\n",
    "        self.towers = tf.Variable(towersInit(shape=(nTasks, expertDim, hiddenDim), dtype=tf.float32), name=\"towers\")\n",
    "        # towers out\n",
    "        towersOut = tf.initializers.truncated_normal(mean=0.0, stddev=1.0)\n",
    "        self.outs = tf.Variable(towersOut(shape=(nTasks, hiddenDim, 1), dtype=tf.float32), name=\"outs\")\n",
    "        # target loss weights, (1, nTasks)\n",
    "        self.tasksWeights = tf.constant([[1.0, 1.0]])\n",
    "        assert self.tasksWeights.shape[1] == nTasks\n",
    "        self.trainableWeights = [\n",
    "            self.experts,\n",
    "            self.gates,\n",
    "            self.towers,\n",
    "            self.outs\n",
    "        ]\n",
    "        self.opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    def __call__(self, input, labels=None):\n",
    "        \"\"\"\n",
    "            @input: (batch, inputDim)\n",
    "            @labels: (batch, nTasks)\n",
    "        \"\"\"\n",
    "        # (batch, 1, 1, inputDim), second dim is for expert broadcast, third dim is for matmul\n",
    "        input = tf.expand_dims(tf.expand_dims(input, axis=1), axis=1)\n",
    "\n",
    "        # (batch, 1, 1, inputDim) X (nExperts, inputDim, expertDim) -> (batch, nExperts, 1, expertDim) -> (batch, 1, nExperts, expertDim)\n",
    "        expertsOut = tf.transpose(tf.matmul(input, model.experts), [0, 2, 1, 3])\n",
    "\n",
    "        # (batch, 1, 1, inputDim) X (nTasks, inputDim, nExperts) -> (batch, nTasks, 1, nExperts)\n",
    "        # TODO: mask for seq padding\n",
    "        weights = tf.nn.softmax(tf.matmul(input, model.gates), 3)\n",
    "        # (batch, nTasks, 1, nExperts) X (batch, 1, nExperts, expertDim) -> (batch, nTasks, 1, expertDim)\n",
    "        towersIn = tf.matmul(weights, expertsOut)\n",
    "        # (batch, nTasks, 1, expertDim) X (nTasks, expertDim, hiddenDim) -> (batch, nTasks, 1, hiddenDim)\n",
    "        towersHidden = tf.nn.relu(tf.matmul(towersIn, model.towers))\n",
    "        # (batch, nTasks, 1, hiddenDim) X (nTasks, hiddenDim, 1) -> (batch, nTasks, 1, 1) -> (batch, nTasks)\n",
    "        outs = tf.squeeze(tf.matmul(towersHidden, model.outs), axis=[2, 3])\n",
    "\n",
    "        if labels is not None:\n",
    "            # train\n",
    "            # (batch, nTasks)\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels, outs)\n",
    "            # loss fusion, (1, nTasks) X (batch, nTasks, 1) -> (batch, 1, 1) -> (batch, )\n",
    "            losses = tf.squeeze(tf.matmul(model.tasksWeights, tf.expand_dims(losses, axis=2)), axis=[1, 2])\n",
    "            losses = tf.reduce_mean(losses)\n",
    "            return losses\n",
    "        else:\n",
    "            # infer\n",
    "            return tf.nn.sigmoid(outs)\n",
    "\n",
    "inputDim = 499\n",
    "expertDim = 256\n",
    "nExperts = 3\n",
    "nTasks = 2\n",
    "hiddenDim = 128\n",
    "\n",
    "model = MMOEDense(nTasks, nExperts, inputDim, expertDim, hiddenDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 000 | step: 000010 | epoch avg loss: 55.0350\n",
      "| epoch: 000 | step: 000020 | epoch avg loss: 55.3614\n",
      "| epoch: 000 | step: 000030 | epoch avg loss: 51.9114\n",
      "| epoch: 000 | step: 000040 | epoch avg loss: 48.8507\n",
      "| epoch: 000 | step: 000050 | epoch avg loss: 47.8450\n",
      "| epoch: 000 | step: 000060 | epoch avg loss: 46.1185\n",
      "| epoch: 000 | step: 000070 | epoch avg loss: 43.9611\n",
      "| epoch: 000 | step: 000080 | epoch avg loss: 42.0298\n",
      "| epoch: 000 | step: 000090 | epoch avg loss: 40.5806\n",
      "| epoch: 000 | step: 000100 | epoch avg loss: 39.0644\n",
      "| epoch: 000 | step: 000110 | epoch avg loss: 38.7356\n",
      "| epoch: 000 | step: 000120 | epoch avg loss: 39.5902\n",
      "| epoch: 000 | step: 000130 | epoch avg loss: 40.6469\n",
      "| epoch: 000 | step: 000140 | epoch avg loss: 41.6594\n",
      "| epoch: 000 | step: 000150 | epoch avg loss: 42.0776\n",
      "| epoch: 000 | step: 000160 | epoch avg loss: 42.4517\n",
      "| epoch: 000 | step: 000170 | epoch avg loss: 43.0969\n",
      "| epoch: 000 | step: 000180 | epoch avg loss: 43.4966\n",
      "| epoch: 000 | step: 000190 | epoch avg loss: 43.9002\n",
      "| epoch: 001 | step: 000010 | epoch avg loss: 28.2614\n",
      "| epoch: 001 | step: 000020 | epoch avg loss: 26.4796\n",
      "| epoch: 001 | step: 000030 | epoch avg loss: 23.9986\n",
      "| epoch: 001 | step: 000040 | epoch avg loss: 23.1213\n",
      "| epoch: 001 | step: 000050 | epoch avg loss: 22.4796\n",
      "| epoch: 001 | step: 000060 | epoch avg loss: 22.1478\n",
      "| epoch: 001 | step: 000070 | epoch avg loss: 21.7893\n",
      "| epoch: 001 | step: 000080 | epoch avg loss: 20.8460\n",
      "| epoch: 001 | step: 000090 | epoch avg loss: 20.2152\n",
      "| epoch: 001 | step: 000100 | epoch avg loss: 19.6061\n",
      "| epoch: 001 | step: 000110 | epoch avg loss: 19.4339\n",
      "| epoch: 001 | step: 000120 | epoch avg loss: 19.5623\n",
      "| epoch: 001 | step: 000130 | epoch avg loss: 19.9529\n",
      "| epoch: 001 | step: 000140 | epoch avg loss: 20.0905\n",
      "| epoch: 001 | step: 000150 | epoch avg loss: 20.0352\n",
      "| epoch: 001 | step: 000160 | epoch avg loss: 20.1496\n",
      "| epoch: 001 | step: 000170 | epoch avg loss: 20.3567\n",
      "| epoch: 001 | step: 000180 | epoch avg loss: 20.4543\n",
      "| epoch: 001 | step: 000190 | epoch avg loss: 20.6115\n",
      "| epoch: 002 | step: 000010 | epoch avg loss: 15.0937\n",
      "| epoch: 002 | step: 000020 | epoch avg loss: 14.3564\n",
      "| epoch: 002 | step: 000030 | epoch avg loss: 13.2003\n",
      "| epoch: 002 | step: 000040 | epoch avg loss: 12.8009\n",
      "| epoch: 002 | step: 000050 | epoch avg loss: 12.4731\n",
      "| epoch: 002 | step: 000060 | epoch avg loss: 12.2720\n",
      "| epoch: 002 | step: 000070 | epoch avg loss: 12.0837\n",
      "| epoch: 002 | step: 000080 | epoch avg loss: 11.8280\n",
      "| epoch: 002 | step: 000090 | epoch avg loss: 11.7542\n",
      "| epoch: 002 | step: 000100 | epoch avg loss: 11.4872\n",
      "| epoch: 002 | step: 000110 | epoch avg loss: 11.3074\n",
      "| epoch: 002 | step: 000120 | epoch avg loss: 11.3548\n",
      "| epoch: 002 | step: 000130 | epoch avg loss: 11.4580\n",
      "| epoch: 002 | step: 000140 | epoch avg loss: 11.6195\n",
      "| epoch: 002 | step: 000150 | epoch avg loss: 11.6524\n",
      "| epoch: 002 | step: 000160 | epoch avg loss: 11.6981\n",
      "| epoch: 002 | step: 000170 | epoch avg loss: 11.8371\n",
      "| epoch: 002 | step: 000180 | epoch avg loss: 11.8545\n",
      "| epoch: 002 | step: 000190 | epoch avg loss: 11.8630\n",
      "| epoch: 003 | step: 000010 | epoch avg loss: 9.4952\n",
      "| epoch: 003 | step: 000020 | epoch avg loss: 9.7611\n",
      "| epoch: 003 | step: 000030 | epoch avg loss: 9.0713\n",
      "| epoch: 003 | step: 000040 | epoch avg loss: 9.1881\n",
      "| epoch: 003 | step: 000050 | epoch avg loss: 8.8921\n",
      "| epoch: 003 | step: 000060 | epoch avg loss: 8.7183\n",
      "| epoch: 003 | step: 000070 | epoch avg loss: 8.4767\n",
      "| epoch: 003 | step: 000080 | epoch avg loss: 8.3687\n",
      "| epoch: 003 | step: 000090 | epoch avg loss: 8.2736\n",
      "| epoch: 003 | step: 000100 | epoch avg loss: 8.1600\n",
      "| epoch: 003 | step: 000110 | epoch avg loss: 8.1965\n",
      "| epoch: 003 | step: 000120 | epoch avg loss: 8.1050\n",
      "| epoch: 003 | step: 000130 | epoch avg loss: 8.1631\n",
      "| epoch: 003 | step: 000140 | epoch avg loss: 8.2214\n",
      "| epoch: 003 | step: 000150 | epoch avg loss: 8.1225\n",
      "| epoch: 003 | step: 000160 | epoch avg loss: 8.1705\n",
      "| epoch: 003 | step: 000170 | epoch avg loss: 8.2209\n",
      "| epoch: 003 | step: 000180 | epoch avg loss: 8.2895\n",
      "| epoch: 003 | step: 000190 | epoch avg loss: 8.2498\n",
      "| epoch: 004 | step: 000010 | epoch avg loss: 6.4689\n",
      "| epoch: 004 | step: 000020 | epoch avg loss: 6.6401\n",
      "| epoch: 004 | step: 000030 | epoch avg loss: 5.9888\n",
      "| epoch: 004 | step: 000040 | epoch avg loss: 6.2502\n",
      "| epoch: 004 | step: 000050 | epoch avg loss: 6.2421\n",
      "| epoch: 004 | step: 000060 | epoch avg loss: 6.3712\n",
      "| epoch: 004 | step: 000070 | epoch avg loss: 6.1677\n",
      "| epoch: 004 | step: 000080 | epoch avg loss: 6.0730\n",
      "| epoch: 004 | step: 000090 | epoch avg loss: 5.9882\n",
      "| epoch: 004 | step: 000100 | epoch avg loss: 5.8294\n",
      "| epoch: 004 | step: 000110 | epoch avg loss: 5.7928\n",
      "| epoch: 004 | step: 000120 | epoch avg loss: 5.7540\n",
      "| epoch: 004 | step: 000130 | epoch avg loss: 5.7362\n",
      "| epoch: 004 | step: 000140 | epoch avg loss: 5.7785\n",
      "| epoch: 004 | step: 000150 | epoch avg loss: 5.7665\n",
      "| epoch: 004 | step: 000160 | epoch avg loss: 5.7636\n",
      "| epoch: 004 | step: 000170 | epoch avg loss: 5.8021\n",
      "| epoch: 004 | step: 000180 | epoch avg loss: 5.8166\n",
      "| epoch: 004 | step: 000190 | epoch avg loss: 5.7993\n",
      "| epoch: 005 | step: 000010 | epoch avg loss: 4.8031\n",
      "| epoch: 005 | step: 000020 | epoch avg loss: 5.0551\n",
      "| epoch: 005 | step: 000030 | epoch avg loss: 4.4734\n",
      "| epoch: 005 | step: 000040 | epoch avg loss: 4.6196\n",
      "| epoch: 005 | step: 000050 | epoch avg loss: 4.5706\n",
      "| epoch: 005 | step: 000060 | epoch avg loss: 4.5891\n",
      "| epoch: 005 | step: 000070 | epoch avg loss: 4.5091\n",
      "| epoch: 005 | step: 000080 | epoch avg loss: 4.4231\n",
      "| epoch: 005 | step: 000090 | epoch avg loss: 4.3964\n",
      "| epoch: 005 | step: 000100 | epoch avg loss: 4.2796\n",
      "| epoch: 005 | step: 000110 | epoch avg loss: 4.2909\n",
      "| epoch: 005 | step: 000120 | epoch avg loss: 4.2661\n",
      "| epoch: 005 | step: 000130 | epoch avg loss: 4.2280\n",
      "| epoch: 005 | step: 000140 | epoch avg loss: 4.2795\n",
      "| epoch: 005 | step: 000150 | epoch avg loss: 4.2340\n",
      "| epoch: 005 | step: 000160 | epoch avg loss: 4.2819\n",
      "| epoch: 005 | step: 000170 | epoch avg loss: 4.3132\n",
      "| epoch: 005 | step: 000180 | epoch avg loss: 4.3234\n",
      "| epoch: 005 | step: 000190 | epoch avg loss: 4.3012\n",
      "| epoch: 006 | step: 000010 | epoch avg loss: 4.0471\n",
      "| epoch: 006 | step: 000020 | epoch avg loss: 3.7069\n",
      "| epoch: 006 | step: 000030 | epoch avg loss: 3.4939\n",
      "| epoch: 006 | step: 000040 | epoch avg loss: 3.6670\n",
      "| epoch: 006 | step: 000050 | epoch avg loss: 3.7306\n",
      "| epoch: 006 | step: 000060 | epoch avg loss: 3.7280\n",
      "| epoch: 006 | step: 000070 | epoch avg loss: 3.7118\n",
      "| epoch: 006 | step: 000080 | epoch avg loss: 3.6759\n",
      "| epoch: 006 | step: 000090 | epoch avg loss: 3.6737\n",
      "| epoch: 006 | step: 000100 | epoch avg loss: 3.6696\n",
      "| epoch: 006 | step: 000110 | epoch avg loss: 3.6601\n",
      "| epoch: 006 | step: 000120 | epoch avg loss: 3.6053\n",
      "| epoch: 006 | step: 000130 | epoch avg loss: 3.5901\n",
      "| epoch: 006 | step: 000140 | epoch avg loss: 3.5728\n",
      "| epoch: 006 | step: 000150 | epoch avg loss: 3.5054\n",
      "| epoch: 006 | step: 000160 | epoch avg loss: 3.4767\n",
      "| epoch: 006 | step: 000170 | epoch avg loss: 3.4802\n",
      "| epoch: 006 | step: 000180 | epoch avg loss: 3.4897\n",
      "| epoch: 006 | step: 000190 | epoch avg loss: 3.4671\n",
      "| epoch: 007 | step: 000010 | epoch avg loss: 2.9709\n",
      "| epoch: 007 | step: 000020 | epoch avg loss: 2.8160\n",
      "| epoch: 007 | step: 000030 | epoch avg loss: 2.7683\n",
      "| epoch: 007 | step: 000040 | epoch avg loss: 2.9181\n",
      "| epoch: 007 | step: 000050 | epoch avg loss: 2.8644\n",
      "| epoch: 007 | step: 000060 | epoch avg loss: 2.8558\n",
      "| epoch: 007 | step: 000070 | epoch avg loss: 2.8554\n",
      "| epoch: 007 | step: 000080 | epoch avg loss: 2.8460\n",
      "| epoch: 007 | step: 000090 | epoch avg loss: 2.8313\n",
      "| epoch: 007 | step: 000100 | epoch avg loss: 2.7973\n",
      "| epoch: 007 | step: 000110 | epoch avg loss: 2.7710\n",
      "| epoch: 007 | step: 000120 | epoch avg loss: 2.7582\n",
      "| epoch: 007 | step: 000130 | epoch avg loss: 2.7444\n",
      "| epoch: 007 | step: 000140 | epoch avg loss: 2.7489\n",
      "| epoch: 007 | step: 000150 | epoch avg loss: 2.7209\n",
      "| epoch: 007 | step: 000160 | epoch avg loss: 2.7340\n",
      "| epoch: 007 | step: 000170 | epoch avg loss: 2.7235\n",
      "| epoch: 007 | step: 000180 | epoch avg loss: 2.7204\n",
      "| epoch: 007 | step: 000190 | epoch avg loss: 2.7243\n",
      "| epoch: 008 | step: 000010 | epoch avg loss: 2.9506\n",
      "| epoch: 008 | step: 000020 | epoch avg loss: 2.7401\n",
      "| epoch: 008 | step: 000030 | epoch avg loss: 2.5504\n",
      "| epoch: 008 | step: 000040 | epoch avg loss: 2.5758\n",
      "| epoch: 008 | step: 000050 | epoch avg loss: 2.6066\n",
      "| epoch: 008 | step: 000060 | epoch avg loss: 2.6267\n",
      "| epoch: 008 | step: 000070 | epoch avg loss: 2.5170\n",
      "| epoch: 008 | step: 000080 | epoch avg loss: 2.4751\n",
      "| epoch: 008 | step: 000090 | epoch avg loss: 2.4412\n",
      "| epoch: 008 | step: 000100 | epoch avg loss: 2.3592\n",
      "| epoch: 008 | step: 000110 | epoch avg loss: 2.3333\n",
      "| epoch: 008 | step: 000120 | epoch avg loss: 2.3358\n",
      "| epoch: 008 | step: 000130 | epoch avg loss: 2.3312\n",
      "| epoch: 008 | step: 000140 | epoch avg loss: 2.3234\n",
      "| epoch: 008 | step: 000150 | epoch avg loss: 2.2815\n",
      "| epoch: 008 | step: 000160 | epoch avg loss: 2.2830\n",
      "| epoch: 008 | step: 000170 | epoch avg loss: 2.2882\n",
      "| epoch: 008 | step: 000180 | epoch avg loss: 2.2897\n",
      "| epoch: 008 | step: 000190 | epoch avg loss: 2.2739\n",
      "| epoch: 009 | step: 000010 | epoch avg loss: 1.8889\n",
      "| epoch: 009 | step: 000020 | epoch avg loss: 1.7568\n",
      "| epoch: 009 | step: 000030 | epoch avg loss: 1.5726\n",
      "| epoch: 009 | step: 000040 | epoch avg loss: 1.7249\n",
      "| epoch: 009 | step: 000050 | epoch avg loss: 1.7391\n",
      "| epoch: 009 | step: 000060 | epoch avg loss: 1.6870\n",
      "| epoch: 009 | step: 000070 | epoch avg loss: 1.6462\n",
      "| epoch: 009 | step: 000080 | epoch avg loss: 1.6890\n",
      "| epoch: 009 | step: 000090 | epoch avg loss: 1.7690\n",
      "| epoch: 009 | step: 000100 | epoch avg loss: 1.7337\n",
      "| epoch: 009 | step: 000110 | epoch avg loss: 1.7368\n",
      "| epoch: 009 | step: 000120 | epoch avg loss: 1.6895\n",
      "| epoch: 009 | step: 000130 | epoch avg loss: 1.7022\n",
      "| epoch: 009 | step: 000140 | epoch avg loss: 1.7131\n",
      "| epoch: 009 | step: 000150 | epoch avg loss: 1.6869\n",
      "| epoch: 009 | step: 000160 | epoch avg loss: 1.7050\n",
      "| epoch: 009 | step: 000170 | epoch avg loss: 1.7142\n",
      "| epoch: 009 | step: 000180 | epoch avg loss: 1.7225\n",
      "| epoch: 009 | step: 000190 | epoch avg loss: 1.7326\n",
      "| epoch: 010 | step: 000010 | epoch avg loss: 1.3262\n",
      "| epoch: 010 | step: 000020 | epoch avg loss: 1.6308\n",
      "| epoch: 010 | step: 000030 | epoch avg loss: 1.5488\n",
      "| epoch: 010 | step: 000040 | epoch avg loss: 1.5206\n",
      "| epoch: 010 | step: 000050 | epoch avg loss: 1.4996\n",
      "| epoch: 010 | step: 000060 | epoch avg loss: 1.4638\n",
      "| epoch: 010 | step: 000070 | epoch avg loss: 1.4775\n",
      "| epoch: 010 | step: 000080 | epoch avg loss: 1.4648\n",
      "| epoch: 010 | step: 000090 | epoch avg loss: 1.4158\n",
      "| epoch: 010 | step: 000100 | epoch avg loss: 1.4347\n",
      "| epoch: 010 | step: 000110 | epoch avg loss: 1.4574\n",
      "| epoch: 010 | step: 000120 | epoch avg loss: 1.4526\n",
      "| epoch: 010 | step: 000130 | epoch avg loss: 1.4494\n",
      "| epoch: 010 | step: 000140 | epoch avg loss: 1.4277\n",
      "| epoch: 010 | step: 000150 | epoch avg loss: 1.4271\n",
      "| epoch: 010 | step: 000160 | epoch avg loss: 1.4083\n",
      "| epoch: 010 | step: 000170 | epoch avg loss: 1.4092\n",
      "| epoch: 010 | step: 000180 | epoch avg loss: 1.4075\n",
      "| epoch: 010 | step: 000190 | epoch avg loss: 1.4046\n",
      "| epoch: 011 | step: 000010 | epoch avg loss: 1.3497\n",
      "| epoch: 011 | step: 000020 | epoch avg loss: 1.4990\n",
      "| epoch: 011 | step: 000030 | epoch avg loss: 1.3653\n",
      "| epoch: 011 | step: 000040 | epoch avg loss: 1.5066\n",
      "| epoch: 011 | step: 000050 | epoch avg loss: 1.4683\n",
      "| epoch: 011 | step: 000060 | epoch avg loss: 1.4335\n",
      "| epoch: 011 | step: 000070 | epoch avg loss: 1.3895\n",
      "| epoch: 011 | step: 000080 | epoch avg loss: 1.4308\n",
      "| epoch: 011 | step: 000090 | epoch avg loss: 1.4275\n",
      "| epoch: 011 | step: 000100 | epoch avg loss: 1.3911\n",
      "| epoch: 011 | step: 000110 | epoch avg loss: 1.3832\n",
      "| epoch: 011 | step: 000120 | epoch avg loss: 1.3763\n",
      "| epoch: 011 | step: 000130 | epoch avg loss: 1.3731\n",
      "| epoch: 011 | step: 000140 | epoch avg loss: 1.3902\n",
      "| epoch: 011 | step: 000150 | epoch avg loss: 1.3871\n",
      "| epoch: 011 | step: 000160 | epoch avg loss: 1.3758\n",
      "| epoch: 011 | step: 000170 | epoch avg loss: 1.3754\n",
      "| epoch: 011 | step: 000180 | epoch avg loss: 1.3848\n",
      "| epoch: 011 | step: 000190 | epoch avg loss: 1.3750\n",
      "| epoch: 012 | step: 000010 | epoch avg loss: 1.4359\n",
      "| epoch: 012 | step: 000020 | epoch avg loss: 1.3884\n",
      "| epoch: 012 | step: 000030 | epoch avg loss: 1.2761\n",
      "| epoch: 012 | step: 000040 | epoch avg loss: 1.2939\n",
      "| epoch: 012 | step: 000050 | epoch avg loss: 1.3011\n",
      "| epoch: 012 | step: 000060 | epoch avg loss: 1.2952\n",
      "| epoch: 012 | step: 000070 | epoch avg loss: 1.2709\n",
      "| epoch: 012 | step: 000080 | epoch avg loss: 1.2401\n",
      "| epoch: 012 | step: 000090 | epoch avg loss: 1.2522\n",
      "| epoch: 012 | step: 000100 | epoch avg loss: 1.2478\n",
      "| epoch: 012 | step: 000110 | epoch avg loss: 1.2793\n",
      "| epoch: 012 | step: 000120 | epoch avg loss: 1.2732\n",
      "| epoch: 012 | step: 000130 | epoch avg loss: 1.2776\n",
      "| epoch: 012 | step: 000140 | epoch avg loss: 1.2776\n",
      "| epoch: 012 | step: 000150 | epoch avg loss: 1.2674\n",
      "| epoch: 012 | step: 000160 | epoch avg loss: 1.2707\n",
      "| epoch: 012 | step: 000170 | epoch avg loss: 1.2772\n",
      "| epoch: 012 | step: 000180 | epoch avg loss: 1.2645\n",
      "| epoch: 012 | step: 000190 | epoch avg loss: 1.2615\n",
      "| epoch: 013 | step: 000010 | epoch avg loss: 1.1198\n",
      "| epoch: 013 | step: 000020 | epoch avg loss: 1.1701\n",
      "| epoch: 013 | step: 000030 | epoch avg loss: 1.0849\n",
      "| epoch: 013 | step: 000040 | epoch avg loss: 1.1266\n",
      "| epoch: 013 | step: 000050 | epoch avg loss: 1.1397\n",
      "| epoch: 013 | step: 000060 | epoch avg loss: 1.1532\n",
      "| epoch: 013 | step: 000070 | epoch avg loss: 1.1467\n",
      "| epoch: 013 | step: 000080 | epoch avg loss: 1.1781\n",
      "| epoch: 013 | step: 000090 | epoch avg loss: 1.1620\n",
      "| epoch: 013 | step: 000100 | epoch avg loss: 1.1638\n",
      "| epoch: 013 | step: 000110 | epoch avg loss: 1.1748\n",
      "| epoch: 013 | step: 000120 | epoch avg loss: 1.1677\n",
      "| epoch: 013 | step: 000130 | epoch avg loss: 1.1716\n",
      "| epoch: 013 | step: 000140 | epoch avg loss: 1.1706\n",
      "| epoch: 013 | step: 000150 | epoch avg loss: 1.1675\n",
      "| epoch: 013 | step: 000160 | epoch avg loss: 1.1901\n",
      "| epoch: 013 | step: 000170 | epoch avg loss: 1.1948\n",
      "| epoch: 013 | step: 000180 | epoch avg loss: 1.2013\n",
      "| epoch: 013 | step: 000190 | epoch avg loss: 1.1956\n",
      "| epoch: 014 | step: 000010 | epoch avg loss: 1.0753\n",
      "| epoch: 014 | step: 000020 | epoch avg loss: 1.1312\n",
      "| epoch: 014 | step: 000030 | epoch avg loss: 1.1663\n",
      "| epoch: 014 | step: 000040 | epoch avg loss: 1.1793\n",
      "| epoch: 014 | step: 000050 | epoch avg loss: 1.1593\n",
      "| epoch: 014 | step: 000060 | epoch avg loss: 1.1479\n",
      "| epoch: 014 | step: 000070 | epoch avg loss: 1.1290\n",
      "| epoch: 014 | step: 000080 | epoch avg loss: 1.0887\n",
      "| epoch: 014 | step: 000090 | epoch avg loss: 1.0957\n",
      "| epoch: 014 | step: 000100 | epoch avg loss: 1.0755\n",
      "| epoch: 014 | step: 000110 | epoch avg loss: 1.0705\n",
      "| epoch: 014 | step: 000120 | epoch avg loss: 1.0715\n",
      "| epoch: 014 | step: 000130 | epoch avg loss: 1.0513\n",
      "| epoch: 014 | step: 000140 | epoch avg loss: 1.0596\n",
      "| epoch: 014 | step: 000150 | epoch avg loss: 1.0545\n",
      "| epoch: 014 | step: 000160 | epoch avg loss: 1.0563\n",
      "| epoch: 014 | step: 000170 | epoch avg loss: 1.0618\n",
      "| epoch: 014 | step: 000180 | epoch avg loss: 1.0647\n",
      "| epoch: 014 | step: 000190 | epoch avg loss: 1.0595\n",
      "| epoch: 015 | step: 000010 | epoch avg loss: 1.0013\n",
      "| epoch: 015 | step: 000020 | epoch avg loss: 0.9492\n",
      "| epoch: 015 | step: 000030 | epoch avg loss: 0.8902\n",
      "| epoch: 015 | step: 000040 | epoch avg loss: 1.0228\n",
      "| epoch: 015 | step: 000050 | epoch avg loss: 1.0379\n",
      "| epoch: 015 | step: 000060 | epoch avg loss: 1.0170\n",
      "| epoch: 015 | step: 000070 | epoch avg loss: 0.9705\n",
      "| epoch: 015 | step: 000080 | epoch avg loss: 0.9910\n",
      "| epoch: 015 | step: 000090 | epoch avg loss: 0.9846\n",
      "| epoch: 015 | step: 000100 | epoch avg loss: 0.9681\n",
      "| epoch: 015 | step: 000110 | epoch avg loss: 0.9662\n",
      "| epoch: 015 | step: 000120 | epoch avg loss: 0.9606\n",
      "| epoch: 015 | step: 000130 | epoch avg loss: 0.9542\n",
      "| epoch: 015 | step: 000140 | epoch avg loss: 0.9593\n",
      "| epoch: 015 | step: 000150 | epoch avg loss: 0.9652\n",
      "| epoch: 015 | step: 000160 | epoch avg loss: 0.9699\n",
      "| epoch: 015 | step: 000170 | epoch avg loss: 0.9717\n",
      "| epoch: 015 | step: 000180 | epoch avg loss: 0.9777\n",
      "| epoch: 015 | step: 000190 | epoch avg loss: 0.9706\n",
      "| epoch: 016 | step: 000010 | epoch avg loss: 1.0399\n",
      "| epoch: 016 | step: 000020 | epoch avg loss: 1.1237\n",
      "| epoch: 016 | step: 000030 | epoch avg loss: 0.9905\n",
      "| epoch: 016 | step: 000040 | epoch avg loss: 1.0231\n",
      "| epoch: 016 | step: 000050 | epoch avg loss: 1.0172\n",
      "| epoch: 016 | step: 000060 | epoch avg loss: 1.0528\n",
      "| epoch: 016 | step: 000070 | epoch avg loss: 1.0287\n",
      "| epoch: 016 | step: 000080 | epoch avg loss: 1.0433\n",
      "| epoch: 016 | step: 000090 | epoch avg loss: 1.0477\n",
      "| epoch: 016 | step: 000100 | epoch avg loss: 1.0435\n",
      "| epoch: 016 | step: 000110 | epoch avg loss: 1.0437\n",
      "| epoch: 016 | step: 000120 | epoch avg loss: 1.0276\n",
      "| epoch: 016 | step: 000130 | epoch avg loss: 1.0091\n",
      "| epoch: 016 | step: 000140 | epoch avg loss: 1.0048\n",
      "| epoch: 016 | step: 000150 | epoch avg loss: 1.0060\n",
      "| epoch: 016 | step: 000160 | epoch avg loss: 0.9931\n",
      "| epoch: 016 | step: 000170 | epoch avg loss: 0.9881\n",
      "| epoch: 016 | step: 000180 | epoch avg loss: 1.0087\n",
      "| epoch: 016 | step: 000190 | epoch avg loss: 1.0141\n",
      "| epoch: 017 | step: 000010 | epoch avg loss: 0.9231\n",
      "| epoch: 017 | step: 000020 | epoch avg loss: 1.0557\n",
      "| epoch: 017 | step: 000030 | epoch avg loss: 0.9637\n",
      "| epoch: 017 | step: 000040 | epoch avg loss: 0.9274\n",
      "| epoch: 017 | step: 000050 | epoch avg loss: 0.9257\n",
      "| epoch: 017 | step: 000060 | epoch avg loss: 0.8858\n",
      "| epoch: 017 | step: 000070 | epoch avg loss: 0.8650\n",
      "| epoch: 017 | step: 000080 | epoch avg loss: 0.8715\n",
      "| epoch: 017 | step: 000090 | epoch avg loss: 0.8499\n",
      "| epoch: 017 | step: 000100 | epoch avg loss: 0.8581\n",
      "| epoch: 017 | step: 000110 | epoch avg loss: 0.8841\n",
      "| epoch: 017 | step: 000120 | epoch avg loss: 0.9009\n",
      "| epoch: 017 | step: 000130 | epoch avg loss: 0.9164\n",
      "| epoch: 017 | step: 000140 | epoch avg loss: 0.9105\n",
      "| epoch: 017 | step: 000150 | epoch avg loss: 0.9041\n",
      "| epoch: 017 | step: 000160 | epoch avg loss: 0.9113\n",
      "| epoch: 017 | step: 000170 | epoch avg loss: 0.9111\n",
      "| epoch: 017 | step: 000180 | epoch avg loss: 0.9216\n",
      "| epoch: 017 | step: 000190 | epoch avg loss: 0.9312\n",
      "| epoch: 018 | step: 000010 | epoch avg loss: 0.8542\n",
      "| epoch: 018 | step: 000020 | epoch avg loss: 0.7701\n",
      "| epoch: 018 | step: 000030 | epoch avg loss: 0.7888\n",
      "| epoch: 018 | step: 000040 | epoch avg loss: 0.8452\n",
      "| epoch: 018 | step: 000050 | epoch avg loss: 0.8367\n",
      "| epoch: 018 | step: 000060 | epoch avg loss: 0.8041\n",
      "| epoch: 018 | step: 000070 | epoch avg loss: 0.7785\n",
      "| epoch: 018 | step: 000080 | epoch avg loss: 0.7834\n",
      "| epoch: 018 | step: 000090 | epoch avg loss: 0.7959\n",
      "| epoch: 018 | step: 000100 | epoch avg loss: 0.8074\n",
      "| epoch: 018 | step: 000110 | epoch avg loss: 0.7892\n",
      "| epoch: 018 | step: 000120 | epoch avg loss: 0.7868\n",
      "| epoch: 018 | step: 000130 | epoch avg loss: 0.7856\n",
      "| epoch: 018 | step: 000140 | epoch avg loss: 0.8046\n",
      "| epoch: 018 | step: 000150 | epoch avg loss: 0.8227\n",
      "| epoch: 018 | step: 000160 | epoch avg loss: 0.8265\n",
      "| epoch: 018 | step: 000170 | epoch avg loss: 0.8243\n",
      "| epoch: 018 | step: 000180 | epoch avg loss: 0.8374\n",
      "| epoch: 018 | step: 000190 | epoch avg loss: 0.8359\n",
      "| epoch: 019 | step: 000010 | epoch avg loss: 0.9259\n",
      "| epoch: 019 | step: 000020 | epoch avg loss: 0.9941\n",
      "| epoch: 019 | step: 000030 | epoch avg loss: 0.8764\n",
      "| epoch: 019 | step: 000040 | epoch avg loss: 0.9462\n",
      "| epoch: 019 | step: 000050 | epoch avg loss: 0.9719\n",
      "| epoch: 019 | step: 000060 | epoch avg loss: 0.9713\n",
      "| epoch: 019 | step: 000070 | epoch avg loss: 0.9765\n",
      "| epoch: 019 | step: 000080 | epoch avg loss: 0.9638\n",
      "| epoch: 019 | step: 000090 | epoch avg loss: 0.9588\n",
      "| epoch: 019 | step: 000100 | epoch avg loss: 0.9472\n",
      "| epoch: 019 | step: 000110 | epoch avg loss: 0.9553\n",
      "| epoch: 019 | step: 000120 | epoch avg loss: 0.9736\n",
      "| epoch: 019 | step: 000130 | epoch avg loss: 0.9761\n",
      "| epoch: 019 | step: 000140 | epoch avg loss: 0.9876\n",
      "| epoch: 019 | step: 000150 | epoch avg loss: 1.0297\n",
      "| epoch: 019 | step: 000160 | epoch avg loss: 1.0577\n",
      "| epoch: 019 | step: 000170 | epoch avg loss: 1.0680\n",
      "| epoch: 019 | step: 000180 | epoch avg loss: 1.0657\n",
      "| epoch: 019 | step: 000190 | epoch avg loss: 1.0603\n",
      "| epoch: 020 | step: 000010 | epoch avg loss: 1.1095\n",
      "| epoch: 020 | step: 000020 | epoch avg loss: 0.9738\n",
      "| epoch: 020 | step: 000030 | epoch avg loss: 0.9665\n",
      "| epoch: 020 | step: 000040 | epoch avg loss: 0.9997\n",
      "| epoch: 020 | step: 000050 | epoch avg loss: 1.0046\n",
      "| epoch: 020 | step: 000060 | epoch avg loss: 1.0570\n",
      "| epoch: 020 | step: 000070 | epoch avg loss: 1.0550\n",
      "| epoch: 020 | step: 000080 | epoch avg loss: 1.0311\n",
      "| epoch: 020 | step: 000090 | epoch avg loss: 0.9994\n",
      "| epoch: 020 | step: 000100 | epoch avg loss: 0.9899\n",
      "| epoch: 020 | step: 000110 | epoch avg loss: 0.9710\n",
      "| epoch: 020 | step: 000120 | epoch avg loss: 0.9789\n",
      "| epoch: 020 | step: 000130 | epoch avg loss: 0.9819\n",
      "| epoch: 020 | step: 000140 | epoch avg loss: 1.0029\n",
      "| epoch: 020 | step: 000150 | epoch avg loss: 1.0078\n",
      "| epoch: 020 | step: 000160 | epoch avg loss: 1.0216\n",
      "| epoch: 020 | step: 000170 | epoch avg loss: 1.0289\n",
      "| epoch: 020 | step: 000180 | epoch avg loss: 1.0317\n",
      "| epoch: 020 | step: 000190 | epoch avg loss: 1.0304\n",
      "| epoch: 021 | step: 000010 | epoch avg loss: 0.9192\n",
      "| epoch: 021 | step: 000020 | epoch avg loss: 1.1224\n",
      "| epoch: 021 | step: 000030 | epoch avg loss: 1.0047\n",
      "| epoch: 021 | step: 000040 | epoch avg loss: 0.9285\n",
      "| epoch: 021 | step: 000050 | epoch avg loss: 0.9682\n",
      "| epoch: 021 | step: 000060 | epoch avg loss: 0.9834\n",
      "| epoch: 021 | step: 000070 | epoch avg loss: 0.9741\n",
      "| epoch: 021 | step: 000080 | epoch avg loss: 1.0128\n",
      "| epoch: 021 | step: 000090 | epoch avg loss: 1.0006\n",
      "| epoch: 021 | step: 000100 | epoch avg loss: 0.9999\n",
      "| epoch: 021 | step: 000110 | epoch avg loss: 0.9940\n",
      "| epoch: 021 | step: 000120 | epoch avg loss: 0.9968\n",
      "| epoch: 021 | step: 000130 | epoch avg loss: 0.9957\n",
      "| epoch: 021 | step: 000140 | epoch avg loss: 0.9745\n",
      "| epoch: 021 | step: 000150 | epoch avg loss: 0.9652\n",
      "| epoch: 021 | step: 000160 | epoch avg loss: 0.9640\n",
      "| epoch: 021 | step: 000170 | epoch avg loss: 0.9655\n",
      "| epoch: 021 | step: 000180 | epoch avg loss: 0.9834\n",
      "| epoch: 021 | step: 000190 | epoch avg loss: 0.9898\n",
      "| epoch: 022 | step: 000010 | epoch avg loss: 1.1600\n",
      "| epoch: 022 | step: 000020 | epoch avg loss: 1.1779\n",
      "| epoch: 022 | step: 000030 | epoch avg loss: 1.2819\n",
      "| epoch: 022 | step: 000040 | epoch avg loss: 1.2850\n",
      "| epoch: 022 | step: 000050 | epoch avg loss: 1.2122\n",
      "| epoch: 022 | step: 000060 | epoch avg loss: 1.2068\n",
      "| epoch: 022 | step: 000070 | epoch avg loss: 1.2033\n",
      "| epoch: 022 | step: 000080 | epoch avg loss: 1.1781\n",
      "| epoch: 022 | step: 000090 | epoch avg loss: 1.1803\n",
      "| epoch: 022 | step: 000100 | epoch avg loss: 1.1523\n",
      "| epoch: 022 | step: 000110 | epoch avg loss: 1.1350\n",
      "| epoch: 022 | step: 000120 | epoch avg loss: 1.1105\n",
      "| epoch: 022 | step: 000130 | epoch avg loss: 1.0869\n",
      "| epoch: 022 | step: 000140 | epoch avg loss: 1.0724\n",
      "| epoch: 022 | step: 000150 | epoch avg loss: 1.0467\n",
      "| epoch: 022 | step: 000160 | epoch avg loss: 1.0261\n",
      "| epoch: 022 | step: 000170 | epoch avg loss: 1.0057\n",
      "| epoch: 022 | step: 000180 | epoch avg loss: 1.0034\n",
      "| epoch: 022 | step: 000190 | epoch avg loss: 0.9989\n",
      "| epoch: 023 | step: 000010 | epoch avg loss: 0.9873\n",
      "| epoch: 023 | step: 000020 | epoch avg loss: 0.9948\n",
      "| epoch: 023 | step: 000030 | epoch avg loss: 0.9475\n",
      "| epoch: 023 | step: 000040 | epoch avg loss: 1.0056\n",
      "| epoch: 023 | step: 000050 | epoch avg loss: 0.9963\n",
      "| epoch: 023 | step: 000060 | epoch avg loss: 0.9919\n",
      "| epoch: 023 | step: 000070 | epoch avg loss: 0.9596\n",
      "| epoch: 023 | step: 000080 | epoch avg loss: 0.9612\n",
      "| epoch: 023 | step: 000090 | epoch avg loss: 0.9478\n",
      "| epoch: 023 | step: 000100 | epoch avg loss: 0.9204\n",
      "| epoch: 023 | step: 000110 | epoch avg loss: 0.9151\n",
      "| epoch: 023 | step: 000120 | epoch avg loss: 0.9093\n",
      "| epoch: 023 | step: 000130 | epoch avg loss: 0.9076\n",
      "| epoch: 023 | step: 000140 | epoch avg loss: 0.9048\n",
      "| epoch: 023 | step: 000150 | epoch avg loss: 0.8963\n",
      "| epoch: 023 | step: 000160 | epoch avg loss: 0.8765\n",
      "| epoch: 023 | step: 000170 | epoch avg loss: 0.8691\n",
      "| epoch: 023 | step: 000180 | epoch avg loss: 0.8647\n",
      "| epoch: 023 | step: 000190 | epoch avg loss: 0.8656\n",
      "| epoch: 024 | step: 000010 | epoch avg loss: 1.0052\n",
      "| epoch: 024 | step: 000020 | epoch avg loss: 1.0533\n",
      "| epoch: 024 | step: 000030 | epoch avg loss: 0.9686\n",
      "| epoch: 024 | step: 000040 | epoch avg loss: 0.9672\n",
      "| epoch: 024 | step: 000050 | epoch avg loss: 0.9091\n",
      "| epoch: 024 | step: 000060 | epoch avg loss: 0.8922\n",
      "| epoch: 024 | step: 000070 | epoch avg loss: 0.8625\n",
      "| epoch: 024 | step: 000080 | epoch avg loss: 0.8443\n",
      "| epoch: 024 | step: 000090 | epoch avg loss: 0.8262\n",
      "| epoch: 024 | step: 000100 | epoch avg loss: 0.8131\n",
      "| epoch: 024 | step: 000110 | epoch avg loss: 0.7991\n",
      "| epoch: 024 | step: 000120 | epoch avg loss: 0.7928\n",
      "| epoch: 024 | step: 000130 | epoch avg loss: 0.8021\n",
      "| epoch: 024 | step: 000140 | epoch avg loss: 0.8053\n",
      "| epoch: 024 | step: 000150 | epoch avg loss: 0.7968\n",
      "| epoch: 024 | step: 000160 | epoch avg loss: 0.8021\n",
      "| epoch: 024 | step: 000170 | epoch avg loss: 0.7936\n",
      "| epoch: 024 | step: 000180 | epoch avg loss: 0.7919\n",
      "| epoch: 024 | step: 000190 | epoch avg loss: 0.7938\n",
      "| epoch: 025 | step: 000010 | epoch avg loss: 0.7322\n",
      "| epoch: 025 | step: 000020 | epoch avg loss: 0.7718\n",
      "| epoch: 025 | step: 000030 | epoch avg loss: 0.7144\n",
      "| epoch: 025 | step: 000040 | epoch avg loss: 0.7227\n",
      "| epoch: 025 | step: 000050 | epoch avg loss: 0.7031\n",
      "| epoch: 025 | step: 000060 | epoch avg loss: 0.7032\n",
      "| epoch: 025 | step: 000070 | epoch avg loss: 0.6872\n",
      "| epoch: 025 | step: 000080 | epoch avg loss: 0.6679\n",
      "| epoch: 025 | step: 000090 | epoch avg loss: 0.6740\n",
      "| epoch: 025 | step: 000100 | epoch avg loss: 0.6791\n",
      "| epoch: 025 | step: 000110 | epoch avg loss: 0.6779\n",
      "| epoch: 025 | step: 000120 | epoch avg loss: 0.6724\n",
      "| epoch: 025 | step: 000130 | epoch avg loss: 0.6801\n",
      "| epoch: 025 | step: 000140 | epoch avg loss: 0.6890\n",
      "| epoch: 025 | step: 000150 | epoch avg loss: 0.6920\n",
      "| epoch: 025 | step: 000160 | epoch avg loss: 0.6808\n",
      "| epoch: 025 | step: 000170 | epoch avg loss: 0.6903\n",
      "| epoch: 025 | step: 000180 | epoch avg loss: 0.7047\n",
      "| epoch: 025 | step: 000190 | epoch avg loss: 0.7057\n",
      "| epoch: 026 | step: 000010 | epoch avg loss: 0.9538\n",
      "| epoch: 026 | step: 000020 | epoch avg loss: 0.9029\n",
      "| epoch: 026 | step: 000030 | epoch avg loss: 0.8404\n",
      "| epoch: 026 | step: 000040 | epoch avg loss: 0.8288\n",
      "| epoch: 026 | step: 000050 | epoch avg loss: 0.7951\n",
      "| epoch: 026 | step: 000060 | epoch avg loss: 0.7778\n",
      "| epoch: 026 | step: 000070 | epoch avg loss: 0.7476\n",
      "| epoch: 026 | step: 000080 | epoch avg loss: 0.7268\n",
      "| epoch: 026 | step: 000090 | epoch avg loss: 0.7325\n",
      "| epoch: 026 | step: 000100 | epoch avg loss: 0.7306\n",
      "| epoch: 026 | step: 000110 | epoch avg loss: 0.7220\n",
      "| epoch: 026 | step: 000120 | epoch avg loss: 0.7218\n",
      "| epoch: 026 | step: 000130 | epoch avg loss: 0.7390\n",
      "| epoch: 026 | step: 000140 | epoch avg loss: 0.7575\n",
      "| epoch: 026 | step: 000150 | epoch avg loss: 0.7559\n",
      "| epoch: 026 | step: 000160 | epoch avg loss: 0.7552\n",
      "| epoch: 026 | step: 000170 | epoch avg loss: 0.7528\n",
      "| epoch: 026 | step: 000180 | epoch avg loss: 0.7669\n",
      "| epoch: 026 | step: 000190 | epoch avg loss: 0.7741\n",
      "| epoch: 027 | step: 000010 | epoch avg loss: 0.9793\n",
      "| epoch: 027 | step: 000020 | epoch avg loss: 0.8243\n",
      "| epoch: 027 | step: 000030 | epoch avg loss: 0.7566\n",
      "| epoch: 027 | step: 000040 | epoch avg loss: 0.7561\n",
      "| epoch: 027 | step: 000050 | epoch avg loss: 0.7677\n",
      "| epoch: 027 | step: 000060 | epoch avg loss: 0.7743\n",
      "| epoch: 027 | step: 000070 | epoch avg loss: 0.7519\n",
      "| epoch: 027 | step: 000080 | epoch avg loss: 0.7258\n",
      "| epoch: 027 | step: 000090 | epoch avg loss: 0.6991\n",
      "| epoch: 027 | step: 000100 | epoch avg loss: 0.6901\n",
      "| epoch: 027 | step: 000110 | epoch avg loss: 0.6954\n",
      "| epoch: 027 | step: 000120 | epoch avg loss: 0.7004\n",
      "| epoch: 027 | step: 000130 | epoch avg loss: 0.6995\n",
      "| epoch: 027 | step: 000140 | epoch avg loss: 0.7045\n",
      "| epoch: 027 | step: 000150 | epoch avg loss: 0.6886\n",
      "| epoch: 027 | step: 000160 | epoch avg loss: 0.6812\n",
      "| epoch: 027 | step: 000170 | epoch avg loss: 0.6751\n",
      "| epoch: 027 | step: 000180 | epoch avg loss: 0.6826\n",
      "| epoch: 027 | step: 000190 | epoch avg loss: 0.6794\n",
      "| epoch: 028 | step: 000010 | epoch avg loss: 0.6515\n",
      "| epoch: 028 | step: 000020 | epoch avg loss: 0.5881\n",
      "| epoch: 028 | step: 000030 | epoch avg loss: 0.5370\n",
      "| epoch: 028 | step: 000040 | epoch avg loss: 0.5735\n",
      "| epoch: 028 | step: 000050 | epoch avg loss: 0.5997\n",
      "| epoch: 028 | step: 000060 | epoch avg loss: 0.5814\n",
      "| epoch: 028 | step: 000070 | epoch avg loss: 0.5772\n",
      "| epoch: 028 | step: 000080 | epoch avg loss: 0.5721\n",
      "| epoch: 028 | step: 000090 | epoch avg loss: 0.5859\n",
      "| epoch: 028 | step: 000100 | epoch avg loss: 0.6115\n",
      "| epoch: 028 | step: 000110 | epoch avg loss: 0.6041\n",
      "| epoch: 028 | step: 000120 | epoch avg loss: 0.6027\n",
      "| epoch: 028 | step: 000130 | epoch avg loss: 0.5989\n",
      "| epoch: 028 | step: 000140 | epoch avg loss: 0.5890\n",
      "| epoch: 028 | step: 000150 | epoch avg loss: 0.5790\n",
      "| epoch: 028 | step: 000160 | epoch avg loss: 0.5764\n",
      "| epoch: 028 | step: 000170 | epoch avg loss: 0.5697\n",
      "| epoch: 028 | step: 000180 | epoch avg loss: 0.5669\n",
      "| epoch: 028 | step: 000190 | epoch avg loss: 0.5649\n",
      "| epoch: 029 | step: 000010 | epoch avg loss: 0.5504\n",
      "| epoch: 029 | step: 000020 | epoch avg loss: 0.4591\n",
      "| epoch: 029 | step: 000030 | epoch avg loss: 0.4218\n",
      "| epoch: 029 | step: 000040 | epoch avg loss: 0.4690\n",
      "| epoch: 029 | step: 000050 | epoch avg loss: 0.5034\n",
      "| epoch: 029 | step: 000060 | epoch avg loss: 0.5067\n",
      "| epoch: 029 | step: 000070 | epoch avg loss: 0.4980\n",
      "| epoch: 029 | step: 000080 | epoch avg loss: 0.4867\n",
      "| epoch: 029 | step: 000090 | epoch avg loss: 0.4762\n",
      "| epoch: 029 | step: 000100 | epoch avg loss: 0.4735\n",
      "| epoch: 029 | step: 000110 | epoch avg loss: 0.4721\n",
      "| epoch: 029 | step: 000120 | epoch avg loss: 0.4708\n",
      "| epoch: 029 | step: 000130 | epoch avg loss: 0.4660\n",
      "| epoch: 029 | step: 000140 | epoch avg loss: 0.4593\n",
      "| epoch: 029 | step: 000150 | epoch avg loss: 0.4571\n",
      "| epoch: 029 | step: 000160 | epoch avg loss: 0.4538\n",
      "| epoch: 029 | step: 000170 | epoch avg loss: 0.4534\n",
      "| epoch: 029 | step: 000180 | epoch avg loss: 0.4557\n",
      "| epoch: 029 | step: 000190 | epoch avg loss: 0.4599\n"
     ]
    }
   ],
   "source": [
    "def parseSample(sample):\n",
    "    #use the same structure as above; it's kinda an outline of the structure we now want to create\n",
    "    data = {\n",
    "        'fea':tf.io.FixedLenFeature([], tf.string),\n",
    "        'marital' : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'income': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    sample = tf.io.parse_single_example(sample, data)\n",
    "\n",
    "    fea = tf.io.parse_tensor(sample[\"fea\"], out_type=tf.float32)\n",
    "    marital = sample['marital']\n",
    "    income = sample['income']\n",
    "\n",
    "    return fea, marital, income\n",
    "\n",
    "train_batch_size = 256\n",
    "\n",
    "trainData = tf.data.TFRecordDataset(train_data_path).map(parseSample).batch(train_batch_size)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "inputBN = tf.keras.layers.BatchNormalization(axis=1)\n",
    "for epoch in range(EPOCHS):\n",
    "    epochTotalLoss = 0\n",
    "    for step, (fea, marital, income) in enumerate(trainData):\n",
    "        labels = tf.cast(tf.transpose(tf.stack([marital, income]), [1, 0]), tf.float32)\n",
    "        # batch norm for census features:\n",
    "        fea = inputBN(fea, training=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            losses = model(fea, labels)\n",
    "            grads = tape.gradient(losses, model.trainableWeights)\n",
    "            model.opt.apply_gradients(zip(grads, model.trainableWeights))\n",
    "        \n",
    "        epochTotalLoss += losses\n",
    "        epochAvgLoss = epochTotalLoss / (step + 1)\n",
    "\n",
    "        if ((step + 1) % 10 == 0):\n",
    "            print(\"| epoch: {:03d} | step: {:06d} | epoch avg loss: {:.4f}\".format(epoch, step + 1, epochAvgLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9908872195532458 0.9595909108248275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "test_batch_size = 64\n",
    "\n",
    "testData = tf.data.TFRecordDataset(test_data_path).map(parseSample).batch(test_batch_size)\n",
    "\n",
    "maritalLogits, maritalLabels = [], []\n",
    "incomeLogits, incomeLabels = [], []\n",
    "for step, (fea, marital, income) in enumerate(testData):\n",
    "    labels = tf.cast(tf.transpose(tf.stack([marital, income]), [1, 0]), tf.float32)\n",
    "    # batch norm for census features:\n",
    "    fea = inputBN(fea, training=False)\n",
    "    logits = model(fea) # (batch, nTasks)\n",
    "    for logit, label in zip(logits, labels):\n",
    "        maritalLogits.append(logit[0])\n",
    "        maritalLabels.append(label[0])\n",
    "        incomeLogits.append(logit[1])\n",
    "        incomeLabels.append(label[1])\n",
    "\n",
    "maritalAuc = roc_auc_score(maritalLabels, maritalLogits)\n",
    "incomeAuc = roc_auc_score(incomeLabels, incomeLogits)\n",
    "\n",
    "print(maritalAuc, incomeAuc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "252189e587d1e2aeba4a06e91fa71896c7a7f6e22e918b9407c7cde4ef2d5985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
